{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**1a)**\n",
    "Implement a function that computes the binary entropy\n",
    "\n",
    "The function takes as input one feature vector x and the corresponding label y. Binary entropy means binary labels y=0/1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1b)**\n",
    "Implement a function that takes one given feature of a dataset, and finds the best split (the split that\n",
    "minimises the entropy) for the data. It is common to iterate over the data feature values, and calculate\n",
    "the entropy for the subsets that are larger or smaller (or equal) than the current value in the iteration.\n",
    "return the split that minimises the entropy and the corresponding entropy for the split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1c)**\n",
    "Implement a function that takes a dataset (nsamples Ã— nfeatures) and finds the single best split (the split\n",
    "with the least entropy) across all features. That is the function find_best_split. Return the best split\n",
    "found."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1d)**\n",
    "Using the functions you implemented above, write your own version of the Classification Tree algorithm,\n",
    "using recursion. Remember to include a parameter specifying the maximum depth of the tree to prevent\n",
    "overfitting\n",
    "![Ethem univariate tree construction](img/decision_tree.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root node at level 1 . Feature index: 0 , split value 0.5835\n",
      "Left node at level 2 . Feature index: 1 , split value 0.4348\n",
      "Left leaf node at level 3 with value 0\n",
      "Right node at level 3 . Feature index: 1 , split value 0.7322\n",
      "Left node at level 4 . Feature index: 0 , split value 0.4281\n",
      "Left leaf node at level 5 with value 0\n",
      "Right leaf node at level 5 with value 1\n",
      "Right leaf node at level 4 with value 1\n",
      "Right leaf node at level 2 with value 1\n",
      "[-0.2096 -0.0599]\n",
      "0\n",
      "Voila!\n"
     ]
    }
   ],
   "source": [
    "MINIMUM_ENTROPY = 0.1\n",
    "\n",
    "import numpy as np\n",
    "from numpy import log2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "def entropy(y: np.array):\n",
    "    counts = np.bincount(y)\n",
    "    p_0 = counts[0] / len(y)\n",
    "    p_1 = 1 - p_0\n",
    "    log2p0 = 0 if p_0 == 0 else log2(p_0)\n",
    "    log2p1 = 0 if p_1 == 0 else log2(p_1)\n",
    "    return -p_0 * log2p0 - p_1 * log2p1\n",
    "\n",
    "\n",
    "def split_feature(x: np.array, y: np.array):\n",
    "    min_impurity_index = None\n",
    "    min_impurity = sys.float_info.max\n",
    "    # Iterate over all values in the features in x.\n",
    "    i_sorted = np.argsort(x)\n",
    "    x = x[i_sorted]  \n",
    "    y = y[i_sorted]\n",
    "        \n",
    "    for i in range(len(x)-1):\n",
    "        sub_1 = y[:i+1]\n",
    "        sub_2 = y[i+1:]\n",
    "        e_1 = entropy(sub_1)\n",
    "        e_2 = entropy(sub_2)\n",
    "        impurity = e_1 * len(sub_1) / len(x) + e_2 * len(sub_2) / len(x)\n",
    "        if impurity < min_impurity:\n",
    "            min_impurity = impurity\n",
    "            min_impurity_index = i\n",
    "\n",
    "    return x[min_impurity_index], min_impurity\n",
    "\n",
    "def find_best_split(X: np.array, y: np.array):\n",
    "    M = X.shape[1]\n",
    "    min_impurity_global = sys.float_info.max\n",
    "    min_impurity_feature_index = None\n",
    "    split_value = None\n",
    "    for i in range(M):\n",
    "        feature = X[:, i]\n",
    "        value, min_impurity = split_feature(feature, y)\n",
    "        if min_impurity < min_impurity_global:\n",
    "            min_impurity_global = min_impurity\n",
    "            min_impurity_feature_index = i\n",
    "            split_value = value\n",
    "            # There might be a break here if we got a entropy near 0.\n",
    "            if min_impurity_global < 1e-20:\n",
    "                break\n",
    "\n",
    "    return min_impurity_feature_index, split_value\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    # Initialize variables\n",
    "    def __init__(self, level, left):\n",
    "        self.__split_value = None\n",
    "        self.__feature_index = None\n",
    "        self.__left_child = None\n",
    "        self.__right_child = None\n",
    "        self.__value = None\n",
    "        self.__level = level + 1\n",
    "        if left is None:\n",
    "            self.__direction = \"Root\"\n",
    "        else:\n",
    "            self.__direction = \"Left\" if left else \"Right\"\n",
    "\n",
    "    # Implements the \"GenerateTree\"-function from Fig. 9.3 in the book.\n",
    "    def fit(self, data, labels):\n",
    "\n",
    "        if entropy(labels) < MINIMUM_ENTROPY:  # Stopping condition for recursion\n",
    "            # Pick the label that has the majority. To be used if we are a leaf-node\n",
    "            self.__value = 1 if np.mean(labels) > 0.5 else 0\n",
    "            print(self.__direction,\"leaf node at level\",self.__level, \"with value\", self.__value)\n",
    "            return\n",
    "\n",
    "        feature_index, split_value = find_best_split(data, labels)\n",
    "\n",
    "        self.__feature_index = feature_index\n",
    "        self.__split_value = split_value\n",
    "        print(self.__direction,\"node at level\", self.__level, \". Feature index:\", feature_index, \", split value\",split_value)\n",
    "\n",
    "        feature = data[:, feature_index]\n",
    "        indexes_left = np.where(feature <= split_value)\n",
    "        indexes_right = np.where(feature > split_value)\n",
    "\n",
    "        # Create branches\n",
    "        self.__left_child = Tree(self.__level, True)\n",
    "        self.__right_child = Tree(self.__level, False)\n",
    "\n",
    "        # Generate sub-trees.\n",
    "        \n",
    "        indexes_left_ = indexes_left[0]\n",
    "        left_ = data[indexes_left_, :]\n",
    "        if len(left_) > 0 :\n",
    "            self.__left_child.fit(left_, labels[indexes_left])\n",
    "        else : \n",
    "            self.__left_child = None\n",
    "        indexes_right_ = indexes_right[0]\n",
    "        right_ = data[indexes_right_, :]\n",
    "        if len(right_) > 0:\n",
    "            self.__right_child.fit(right_, labels[indexes_right])\n",
    "        else:\n",
    "            self.__right_child = None\n",
    "\n",
    "    # Find leaf corresponding to row\n",
    "    def predict(self, x):\n",
    "        if self.__left_child is None and self.__right_child is None:  # We are a leaf-node\n",
    "            return self.__value\n",
    "\n",
    "        feature_val = x[self.__feature_index]\n",
    "        if feature_val <= self.__split_value:\n",
    "            return self.__left_child.predict(x)\n",
    "        else:\n",
    "            return self.__right_child.predict(x)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data/blobs.csv\", skiprows=1, delimiter=\" \")\n",
    "# y = data[0,:].to_numpy(dtype=int)\n",
    "y = data.iloc[:, 0].to_numpy()\n",
    "X = data.iloc[:, 1:].to_numpy()\n",
    "\n",
    "# y = \n",
    "# X = \n",
    "\n",
    "decision_tree = Tree(0,None)\n",
    "decision_tree.fit(X, y)\n",
    "print(\"Voila!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T17:11:15.829935900Z",
     "start_time": "2023-09-27T17:11:15.810338200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1e)**\n",
    "Test your implementation on the datasets in blobs.csv and flame.csv. Plot the data, and the regions\n",
    "found by the tree.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "test = X[153, :]\n",
    "print(test)\n",
    "print(decision_tree.predict(test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T17:23:39.346284300Z",
     "start_time": "2023-09-27T17:23:39.339242300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1f)** Vizualise tree \n",
    "https://levelup.gitconnected.com/binary-tree-implementation-and-visualization-in-python-2f4782887ca2"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
